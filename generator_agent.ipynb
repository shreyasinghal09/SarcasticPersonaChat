{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gs7-0r3qnDcy"
   },
   "outputs": [],
   "source": [
    "# !pip install langchain_aws\n",
    "# !pip install transformers\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z0B2SFpKxEzG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riyasinghal/miniconda3/envs/cmuf24/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JurYGnPTxNpH",
    "outputId": "3fede089-7258-4887-86cb-6883ced1003c"
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/google/Synthetic-Persona-Chat\n",
    "splits = {'train': 'data/Synthetic-Persona-Chat_train.csv', 'validation': 'data/Synthetic-Persona-Chat_valid.csv', 'test': 'data/Synthetic-Persona-Chat_test.csv'}\n",
    "df_test_persona = pd.read_csv(\"hf://datasets/google/Synthetic-Persona-Chat/\" + splits[\"test\"]) #968 rows\n",
    "\n",
    "# https://huggingface.co/datasets/nikesh66/Sarcasm-dataset\n",
    "df_sarcasm = pd.read_csv(\"hf://datasets/nikesh66/Sarcasm-dataset/sarcasm_tweets.csv\")\n",
    "sarcastic_rows = df_sarcasm[df_sarcasm[\"Sarcasm (yes/no)\"] == \"yes\"] #2500 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hmb_p1nyxq5C"
   },
   "outputs": [],
   "source": [
    "def sample_persona_pair(df):\n",
    "    sampled_row = df.sample(n=1).iloc[0]\n",
    "    return {\n",
    "        \"user1_persona\": sampled_row[\"user 1 personas\"],\n",
    "        \"user2_persona\": sampled_row[\"user 2 personas\"],\n",
    "        \"conversation\": sampled_row[\"Best Generated Conversation\"]\n",
    "    }\n",
    "\n",
    "def sample_sarcastic_examples(df):\n",
    "  # sarcastic_samples = df[\"Tweet\"].sample(n=5, random_state=42).to_list()\n",
    "  sarcastic_samples = df[\"Tweet\"].sample(n=5).to_list()\n",
    "\n",
    "  return sarcastic_samples\n",
    "\n",
    "random_sample = sample_persona_pair(df_test_persona)\n",
    "sarcastic_samples = sample_sarcastic_examples(df_sarcasm)\n",
    "\n",
    "# print(\"User 1 Persona:\", random_sample[\"user1_persona\"])\n",
    "# print(\"User 2 Persona:\", random_sample[\"user2_persona\"])\n",
    "# print(\"Conversation:\", random_sample[\"conversation\"])\n",
    "# print(\"\\n\\n\")\n",
    "# print(sarcastic_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Ms82qnsbmqB1"
   },
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
    "    # model_id = \"meta.llama3-1-8b-instruct-v1:0\",\n",
    "    model_kwargs=dict(temperature=0.9),\n",
    "    aws_access_key_id=\"AKIAQXUIX3RJR2NPZQ47\",\n",
    "    aws_secret_access_key=\"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\",\n",
    "    region_name=\"us-east-1\",\n",
    "    max_tokens = 1000\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\n",
    "#             \"system\",\n",
    "#             \"\"\" Your task is to generate conversations that incorporate some sarcasm in the tone. The conversations must feel natural and reflect the given user profiles, with implicit indications of their traits, interests, or personalities. Follow these guidelines to ensure quality: The conversation must be more than 5 turns and less than 8 turns. The\n",
    "#               conversation must be natural, and not direct copies of their profiles. Sarcasm should appear naturally within the conversation, and not all turns need to include sarcasm.\n",
    "#               Sarcasm can range from playful and lighthearted to sharper, as long as it aligns with the users' personalities. The conversation must be more than 5 turns and less than 8 turns. The conversation must be natural, and not direct copies of their profiles.\"\"\",\n",
    "#                       ),\n",
    "#         (\n",
    "#             \"human\",\n",
    "#             \"\"\"\n",
    "#             Here, we list the profiles of two users, user 1 and user 2, followed by an interesting and natural conversation between user 1 and user 2, which implicitly reflects their user profiles. We also list some sarcastic statements.\n",
    "#               User 1 Profile: {user_1_profile}\n",
    "#               User 2 Profile: {user_2_profile}\n",
    "#               Conversation: {original_conversation}\n",
    "#               Sarcastic Statements: {sarcastic_statements}\n",
    "#               \"\"\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Your task is to generate conversations that incorporate sarcasm naturally and creatively. Each conversation must reflect the personalities, interests, and traits of the given user profiles, using sarcasm in ways that are contextually appropriate and tailored to the users' traits. Follow these guidelines:\n",
    "\n",
    "              1. Natural Flow: The conversation should feel organic and realistic, reflecting the users' personalities implicitly.\n",
    "              2. Sarcasm: Use sarcasm ranging from playful to sharp, as long as it aligns with the tone of the conversation and the users' traits. Not every turn needs to be sarcastic.\n",
    "              3. Creative Adaptation: Use the sarcastic examples provided only for tone inspiration. Do not use the same phrases or replicate their structure directly. Instead, derive unique, contextually fitting sarcasm that feels natural to the conversation.\n",
    "              4. Length: Keep the conversation between 4 and 7 turns.\n",
    "              5. Originality: All elements of the conversation, including sarcastic remarks, should be distinct and avoid direct borrowing from the examples.\n",
    "\n",
    "              Keep in mind: The examples are there to help you understand the tone and style, not to be copied verbatim. Use them as a creative springboard to generate unique and personalized interactions.\"\"\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"\n",
    "            Here, we list the profiles of two users, User 1 and User 2, followed by an interesting and natural conversation between them, which implicitly reflects their user profiles. Sarcasm should be used in a manner inspired by the examples without repeating them.\n",
    "\n",
    "              User 1 Profile: {user_1_profile}\n",
    "              User 2 Profile: {user_2_profile}\n",
    "              Conversation: {original_conversation}\n",
    "              Sarcastic Examples (for tone inspiration only): {sarcastic_statements}\n",
    "              \"\"\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AMWEwf-2l9m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I work in fast food.\n",
      "I dropped out of high school.\n",
      "I am an avid runner.\n",
      "I do not like animals.\n",
      "I live in low income apartments.\n",
      "\n",
      "\n",
      "\n",
      "I want to travel to europe.\n",
      "I like to walk long distances.\n",
      "My favorite type of food is italian.\n",
      "I love reading books about travel.\n",
      "I have a big pet dog.\n",
      "\n",
      "\n",
      "\n",
      "331\n"
     ]
    }
   ],
   "source": [
    "# response = chain.invoke(\n",
    "#     {\n",
    "#         \"user_1_profile\": random_sample[\"user1_persona\"],\n",
    "#         \"user_2_profile\": random_sample[\"user2_persona\"],\n",
    "#         \"original_conversation\": random_sample[\"conversation\"],\n",
    "#         \"sarcastic_statements\": sarcastic_samples\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# print(random_sample[\"user1_persona\"])\n",
    "# print(\"\\n\\n\")\n",
    "# print(random_sample[\"user2_persona\"])\n",
    "# print(\"\\n\\n\")\n",
    "\n",
    "# print(len(random_sample[\"conversation\"].split(\" \")))\n",
    "# # print(\"\\n\\n\")\n",
    "# # print(sarcastic_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QPq3jyGCKkKB",
    "outputId": "49198e3f-4752-4f02-9592-28241eaa9722"
   },
   "outputs": [],
   "source": [
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdbsAjd8Hqqs"
   },
   "source": [
    "Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEaXOWZhHLmK",
    "outputId": "d079f5e5-aaec-4829-e0d0-1e6550aee6d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 data appended\n",
      "2 data appended\n",
      "Processed 2 requests. Sleeping for 60 seconds...\n",
      "3 data appended\n",
      "4 data appended\n",
      "Processed 4 requests. Sleeping for 60 seconds...\n",
      "5 data appended\n",
      "6 data appended\n",
      "Processed 6 requests. Sleeping for 60 seconds...\n",
      "7 data appended\n",
      "8 data appended\n",
      "Processed 8 requests. Sleeping for 60 seconds...\n",
      "9 data appended\n",
      "10 data appended\n",
      "Processed 10 requests. Sleeping for 60 seconds...\n",
      "11 data appended\n",
      "12 data appended\n",
      "Processed 12 requests. Sleeping for 60 seconds...\n",
      "13 data appended\n",
      "14 data appended\n",
      "Processed 14 requests. Sleeping for 60 seconds...\n",
      "15 data appended\n",
      "16 data appended\n",
      "Processed 16 requests. Sleeping for 60 seconds...\n",
      "17 data appended\n",
      "18 data appended\n",
      "Processed 18 requests. Sleeping for 60 seconds...\n",
      "19 data appended\n",
      "20 data appended\n",
      "Processed 20 requests. Sleeping for 60 seconds...\n",
      "21 data appended\n",
      "22 data appended\n",
      "Processed 22 requests. Sleeping for 60 seconds...\n",
      "23 data appended\n",
      "24 data appended\n",
      "Processed 24 requests. Sleeping for 60 seconds...\n",
      "25 data appended\n",
      "26 data appended\n",
      "Processed 26 requests. Sleeping for 60 seconds...\n",
      "27 data appended\n",
      "28 data appended\n",
      "Processed 28 requests. Sleeping for 60 seconds...\n",
      "29 data appended\n",
      "30 data appended\n",
      "Processed 30 requests. Sleeping for 60 seconds...\n",
      "31 data appended\n",
      "32 data appended\n",
      "Processed 32 requests. Sleeping for 60 seconds...\n",
      "33 data appended\n",
      "34 data appended\n",
      "Processed 34 requests. Sleeping for 60 seconds...\n",
      "35 data appended\n",
      "36 data appended\n",
      "Processed 36 requests. Sleeping for 60 seconds...\n",
      "37 data appended\n",
      "38 data appended\n",
      "Processed 38 requests. Sleeping for 60 seconds...\n",
      "39 data appended\n",
      "40 data appended\n",
      "Processed 40 requests. Sleeping for 60 seconds...\n",
      "41 data appended\n",
      "42 data appended\n",
      "Processed 42 requests. Sleeping for 60 seconds...\n",
      "43 data appended\n",
      "44 data appended\n",
      "Processed 44 requests. Sleeping for 60 seconds...\n",
      "45 data appended\n",
      "46 data appended\n",
      "Processed 46 requests. Sleeping for 60 seconds...\n",
      "47 data appended\n",
      "48 data appended\n",
      "Processed 48 requests. Sleeping for 60 seconds...\n",
      "49 data appended\n",
      "50 data appended\n",
      "Processed 50 requests. Sleeping for 60 seconds...\n",
      "51 data appended\n",
      "52 data appended\n",
      "Processed 52 requests. Sleeping for 60 seconds...\n",
      "53 data appended\n",
      "54 data appended\n",
      "Processed 54 requests. Sleeping for 60 seconds...\n",
      "55 data appended\n",
      "56 data appended\n",
      "Processed 56 requests. Sleeping for 60 seconds...\n",
      "57 data appended\n",
      "58 data appended\n",
      "Processed 58 requests. Sleeping for 60 seconds...\n",
      "59 data appended\n",
      "60 data appended\n",
      "Processed 60 requests. Sleeping for 60 seconds...\n",
      "61 data appended\n",
      "62 data appended\n",
      "Processed 62 requests. Sleeping for 60 seconds...\n",
      "63 data appended\n",
      "64 data appended\n",
      "Processed 64 requests. Sleeping for 60 seconds...\n",
      "65 data appended\n",
      "66 data appended\n",
      "Processed 66 requests. Sleeping for 60 seconds...\n",
      "67 data appended\n",
      "68 data appended\n",
      "Processed 68 requests. Sleeping for 60 seconds...\n",
      "69 data appended\n",
      "70 data appended\n",
      "Processed 70 requests. Sleeping for 60 seconds...\n",
      "71 data appended\n",
      "72 data appended\n",
      "Processed 72 requests. Sleeping for 60 seconds...\n",
      "73 data appended\n",
      "74 data appended\n",
      "Processed 74 requests. Sleeping for 60 seconds...\n",
      "75 data appended\n",
      "76 data appended\n",
      "Processed 76 requests. Sleeping for 60 seconds...\n",
      "77 data appended\n",
      "78 data appended\n",
      "Processed 78 requests. Sleeping for 60 seconds...\n",
      "79 data appended\n",
      "80 data appended\n",
      "Processed 80 requests. Sleeping for 60 seconds...\n",
      "81 data appended\n",
      "82 data appended\n",
      "Processed 82 requests. Sleeping for 60 seconds...\n",
      "83 data appended\n",
      "84 data appended\n",
      "Processed 84 requests. Sleeping for 60 seconds...\n",
      "85 data appended\n",
      "86 data appended\n",
      "Processed 86 requests. Sleeping for 60 seconds...\n",
      "87 data appended\n",
      "88 data appended\n",
      "Processed 88 requests. Sleeping for 60 seconds...\n",
      "89 data appended\n",
      "90 data appended\n",
      "Processed 90 requests. Sleeping for 60 seconds...\n",
      "91 data appended\n",
      "92 data appended\n",
      "Processed 92 requests. Sleeping for 60 seconds...\n",
      "93 data appended\n",
      "94 data appended\n",
      "Processed 94 requests. Sleeping for 60 seconds...\n",
      "95 data appended\n",
      "96 data appended\n",
      "Processed 96 requests. Sleeping for 60 seconds...\n",
      "97 data appended\n",
      "98 data appended\n",
      "Processed 98 requests. Sleeping for 60 seconds...\n",
      "99 data appended\n",
      "100 data appended\n",
      "Processed 100 requests. Sleeping for 60 seconds...\n",
      "CSV file has been saved successfully! Time taken: 3941.32 seconds\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for i in range(101):\n",
    "        \n",
    "    random_sample = sample_persona_pair(df_test_persona)\n",
    "    sarcastic_samples = sample_sarcastic_examples(df_sarcasm)\n",
    "\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"user_1_profile\": random_sample[\"user1_persona\"],\n",
    "            \"user_2_profile\": random_sample[\"user2_persona\"],\n",
    "            \"original_conversation\": random_sample[\"conversation\"],\n",
    "            \"sarcastic_statements\": sarcastic_samples\n",
    "        }\n",
    "    )\n",
    "    generated_conversation = response.content\n",
    "\n",
    "    data.append({\n",
    "        \"User_1_Profile\": random_sample[\"user1_persona\"],\n",
    "        \"User_2_Profile\": random_sample[\"user2_persona\"],\n",
    "        \"Original_Conversation\": random_sample[\"conversation\"],\n",
    "        \"Sarcastic_Samples\": sarcastic_samples,\n",
    "        \"Generated_Conversation\": generated_conversation\n",
    "    })\n",
    "\n",
    "    print(str(i+1)+\" data appended\")\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f\"Processed {i + 1} requests. Sleeping for 60 seconds...\")\n",
    "        time.sleep(65)\n",
    "\n",
    "    \n",
    "\n",
    "df_output = pd.DataFrame(data)\n",
    "df_output.to_csv(\"generated_datasets/experiment1/mistral_generator.csv\", index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"CSV file has been saved successfully! Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvZVxLFDHfri"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cmuf24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
